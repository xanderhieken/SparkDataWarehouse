{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Data Warehouse with Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and start a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "warehouse_dir = 'PATH/TO/YOUR/DATA/WAREHOUSE' #filepath where you want your warehouse created\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Building a Data Warehouse\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_dir) \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseCompressedOops\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of corresponding files to combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp is a dictionary of file paths that correspond to each other\n",
    "fp = {'2017/places.csv':'2018/places.csv',\n",
    "      '2017/congressional_district.csv':'2018/congressional_district.csv',\n",
    "      '2017/core_based_statistical_areas.csv':'2018/core_based_statistical_areas.csv',\n",
    "      '2017/counties.csv':'2018/counties.csv',\n",
    "      '2017/county_subdivisions.csv':'2018/county_subdivisions.csv',\n",
    "      '2017/elementary_schools.csv':'2018/elementary_schools.csv',\n",
    "      '2017/tracts.csv':'2018/tracts.csv',\n",
    "      '2017/unified_school_districts.csv':'2018/unified_school_districts.csv',\n",
    "      '2017/urban_areas.csv':'2018/urban_areas.csv',\n",
    "      '2017/zip_code_tabulation_areas.csv':'2018/zip_code_tabulation_areas.csv',\n",
    "      '2017/secondary_schools.csv':'2018/secondary_schools.csv'\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through all the files we just listed, isolate table names, and create the data warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in fp:\n",
    "    df1 = spark.read.load(i, format='csv', sep=',', inferSchema=True, header=True) # loads 2017 files\n",
    "    df2 = spark.read.load(fp[i], format='csv', sep=',', inferSchema=True, header=True) #loads 2018 files\n",
    "    \n",
    "    isolateTable = re.search('2017/(.*).csv', i) # isolates the string I want to use as the table name\n",
    "    tableName = isolateTable.group(1) # sets tableName to whatever is between '2017/' and '.csv' in the file path\n",
    "    \n",
    "    df = df1.unionAll(df2) # joins both tables \n",
    "    df.write.saveAsTable(tableName, mode = 'overwrite') # saves the newly joined table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before moving on, make sure all of the tables were created and correctly labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now take a look at how many rows are in each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in spark.catalog.listTables():\n",
    "    print(i[0])\n",
    "    spark.sql(\"SELECT COUNT(*) AS row_count FROM \" + i[0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Querying Tables with Spark SQL\n",
    "\n",
    "Now that we have saved the data in our warehouse, we can run some queries in Spark SQL and create a report on school districts for the states of Nebraska and Iowa using the elementary_schools, secondary_schools and unified_school_districts tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NE_IA_counts = spark.sql('SELECT state, year, '\n",
    "              '(SELECT COUNT(*) FROM elementary_schools WHERE state == \"NE\" OR state == \"IA\") AS elementary, '\n",
    "              '(SELECT COUNT(*) FROM secondary_schools WHERE state == \"NE\" OR state == \"IA\") AS secondary, '\n",
    "              'COUNT(*) as unified '\n",
    "              'FROM unified_school_districts '\n",
    "              'WHERE state == \"NE\" OR state == \"IA\" '\n",
    "              'GROUP BY state, year '\n",
    "              'ORDER BY state, year '\n",
    "             )\n",
    "\n",
    "NE_IA_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table contains the number of elementary, secondary, and unified school districts in each state for each year\n",
    "***\n",
    "There are a lot of tables we haven't queried yet, so I would encourage you to play around and get a feel for data exploration in Spark SQL.\n",
    "\n",
    "### MAKE SURE YOU RUN THE FOLLOWING CELL WHEN YOU'RE ALL DONE WITH SPARK\n",
    "*Otherwise, Spark will continue running in the background until you do*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
